---
title: '[小猴偷米第二次技术分享]关于爬虫'
date: 2017-10-20 23:18:22
tags:
---

一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。(From: 百度百科)
<!--more-->

## 基本原理
充当了一个自动浏览器的角色，通过一个控制器调度，不断地发送http请求并处理响应，自动获取需要的资源的程序


![](./WebCrawlerArchitecture.png)



## 关于http

(Hypertext Transfer Protocol)


![](./HTTP_Steps.png)


*Request Message*
```bash
GET /docs/index.html HTTP/1.1    -----> Request line
Host: www.nowhere123.com         -----
Accept: image/gif, image/jpeg, */*   |
Accept-Language: en-us               |----> Request Headers
Accept-Encoding: gzip, deflate       |
User-Agent: xxxxxxxxxxxxxxxxxxx ------

(blank line)
```


*Response Message*
```bash
HTTP/1.1 200 OK
Date: Sun, 18 Oct 2009 08:56:53 GMT
Server: Apache/2.2.14
Last-Modified: Sat, 20 Nov 2004 07:16:26 GMT
Accept-Ranges: bytes
Content-Length: 44
Connection: close
Content-Type: text/html
X-Pad: avoid browser bug

<html><body><h1>It works!</h1></body></html>
```


GET 请求

```bash
GET http://xxxx/signup.html?inviteCode=23333&invitepasswd=233
```
直接向服务器请求数据，类似数据库的query，

不会改变服务器上面的数据

不太安全，参数均暴露在URL中

传递参数长度有所限制

参数保留在浏览器历史中。

编码形式：application/x-www-form-urlencoded


POST 请求

数据由表单传递

请求不会被缓存

请求不会保留在浏览器历史记录中

不能被收藏为书签

请求对数据长度没有要求

编码形式：application/x-www-form-urlencoded

或 multipart/form-data


PUT 请求

通常用于向服务器发送请求，如果URI不存在，

则要求服务器根据请求创建资源，如果存在，

服务器就接受请求内容，

并修改URI资源的原始版本。


DELETE 请求

从服务器上面删除一些东西



## 第一个爬虫
爬取[wikiart](https://www.wikiart.org/)中的图片


审查元素，找到所要资源的位置

构思流程，确定爬取路线

编写程序，获取资源


```bash
downloading Print 1 ...
Print 1 is ok!
downloading Painting ...
Painting is ok!
downloading Symmetry Drawing ...
Symmetry Drawing is ok!
downloading The Sick Child ...
The Sick Child is ok!
downloading Head ...
Head is ok!
...
```


## 关于模拟登陆


通过抓包获取登陆所需要的东西，

如：用户名，密码，某些奇怪的参数

根据请求顺序，构造模拟登陆的程序


举个栗子：模拟登陆[校园信息门户](http://my.seu.edu.cn/)


不如试一下selenium这类的自动运维工具？


## 关于爬虫效率


多进程，多线程，协程以及很厉害的分布式爬虫等等...
下面简单介绍协程，使用tornado框架


最开始的同步爬虫，它是下载完一张图片，下载下一张图片，
假如某张图片下载速度极慢，那么后面的图片就要乖乖排队，
就好比你打开了浏览器，从首页点开了第一个链接，等待链接加载完毕，
看过之后，你回到首页打开了第二个链接继续浏览...


而利用了协程的异步爬虫，就好比你一次性的在首页点击了n个链接，谁先加载出来就去
浏览谁，tornado通过不断的轮询查询各个请求的状态，把其中完成响应的拿出来
运行回调函数进行下载...


```bash
$ python3 synchronousCrawler.py
crawler starts @ 1508392617.898792
crawler ends @ 1508392700.0265799
consume 82.12778782844543
```


```bash
$ python3 tornadoCrawler.py
crawler starts @ 1508392821.250983
crawler ends @ 1508392821.250983
consume 40.57411503791809
```


## 关于反爬
很多网页会限制爬虫的访问，有很多策略，比如：验证码，登陆，限制单位时间的访问次数...
验证码识别(参见[seu_jwc_fker](https://github.com/vhyme/seu-jwc-fker))，登陆(cookie抓包)，ip代理池...



当然，这些都是最简单的爬虫了，

但是万变不离其宗


我们希望爬什么东西？图片，字符...

需要储存？文本，图片，数据库...

遍历条件？深度，广度

筛选资源？解析网页？正则表达，第三方库(bs4)

需要登陆？拿cookie，selenium

爬虫效率？多线程，多进程，协程，爬虫框架(pyspider, scrapy)

爬虫被ban？更换ip，ip池，sleep()去睡觉...


## Coding Time!

![](./2333.jpg)


任务：

编写一个seu-wlan的登陆脚本


一些可能需要的函数
```python
import base64
test = base64(b'test')
```

啰嗦了一堆，代码也很丑，

希望能起到抛砖引玉的作用

谢谢
